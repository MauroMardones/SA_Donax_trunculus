---
title: "![](IEO-logo2.png){width=10cm}"
output:
  bookdown::pdf_document2:
    includes:
      before_body: titulo3.sty
    keep_tex: true
    latex_engine: xelatex
    number_sections: no
    toc: true
    toc_depth: 3
bibliography: Donax.bib
csl: apa.csl
link-citations: yes
linkcolor: blue
indent: no
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \lfoot[\thepage]{}
- \rfoot[]{\thepage}
- \fontsize{12}{22}
- \selectfont
---

\pagebreak


```{r setup1, echo=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = 'center',
                      fig.pos = "H",
                      dev = 'jpeg',
                      dpi = 300)
#XQuartz is a mess, put this in your onload to default to cairo instead
options(bitmapType = "cairo") 
# (https://github.com/tidyverse/ggplot2/issues/2655)
```

```{r echo=TRUE, eval=TRUE}
# List of required packages grouped by functionality
required_packages <- c(
  # Stock assessment + SS3 tools
  "r4ss",         # Reading SS3 output
  "ss3diags",     # Diagnostics for SS3 assessments
  #  Data manipulation and transformation
  "dplyr",        # Core data manipulation
  "tidyverse",    # Collection of core tidy packages
  "purrr",        # Functional programming (map, walk, etc.)
  "plyr",         # Older data manipulation package (conflicts with dplyr)
  "reshape2",     # Data reshaping (e.g. melt/cast)
  #  Visualization and plotting
  "ggplot2",      # Core plotting
  "ggthemes",     # Themes like theme_few()
  "ggpubr",       # ggarrange() and publication-ready visuals
  "grid",         # Grid graphics system
  "png",          # Reading PNG images
  "ggpubr",
  #  Statistical and modeling tools
  "mvtnorm",      # Multivariate normal distributions
  "forecast",     # Time series forecasting and diagnostics
  #  File and path management
  "here",         # Reproducible file paths
  #  Reporting and tables
  "kableExtra"    # Enhanced HTML/LaTeX tables
)
missing <- required_packages[!required_packages %in% installed.packages()[, "Package"]]
if (length(missing)) install.packages(missing)
purrr::walk(required_packages, library, character.only = TRUE)
```


# Context

Understanding the potential bias in stock assessment models is critical to ensuring reliable and precautionary management advice. In this study, we evaluate the bias of the reference model developed for the wedge clam (*Donax trunculus*) fishery in the Gulf of Cádiz. The reference model, implemented in Stock Synthesis (SS3), integrates fishery-independent and fishery-dependent data to estimate population dynamics, including spawning stock biomass (SSB) and recruitment.

To quantify model bias, we conducted a non-parametric bootstrap analysis by resampling the length composition data used in the model fitting process. This approach simulates variability in the observed data and allows for the evaluation of the robustness of model estimates under alternative realizations of the input data. Specifically, we generated 100 bootstrap replicates of the original dataset, refitted the SS3 model to each replicate, and compared the resulting time series of SSB and recruitment to those from the reference model.

The primary objective of this analysis is to identify whether the reference model provides unbiased estimates of key population parameters across years, or if systematic deviations arise when observational error is propagated through the model. Additionally, this bootstrap-based evaluation serves as a diagnostic tool to **detect potential structural issues in the model**, such as misspecified selectivity patterns, natural mortality assumptions, or recruitment dynamics. Identifying such structural problems is essential to refine model assumptions, enhance the realism of population estimates, and increase the reliability of management advice derived from the assessment.



# Methodology

To assess the estimation bias of the Stock Synthesis (SS3) model for *Donax trunculus* population dynamics in the Gulf of Cádiz, we performed a bootstrap resampling procedure. This approach enables the evaluation of bias and uncertainty in model parameter estimates by repeatedly fitting the model to resampled datasets generated from the original data.

### Bootstrap Procedure

1. **Resampling**: We generated 100 bootstrap replicates by sampling with replacement from the original observed datasets (e.g., catch-at-length, survey indices), preserving the structure of the data.

2. **Model Fitting**: For each bootstrap replicate, the SS3 model was executed to estimate parameters such as biomass, fishing mortality, and recruitment. The SS3 executable was called programmatically from R using system calls, allowing automated batch processing.

3. **Bias Calculation**: For each parameter $\theta$, the bootstrap estimate of bias was calculated as:

$$
\hat{Bias}(\hat{\theta}) = \bar{\theta}^* - \hat{\theta}
$$
where $\hat{\theta}$ is the parameter estimate from the original data, and $\bar{\theta}^*$ is the mean of bootstrap estimates across replicates.

4. **Confidence Intervals**: Bootstrap percentile confidence intervals were obtained from the empirical distribution of parameter estimates from the bootstrap replicates.

## Parametric Bootstrapping Procedure in Stock Synthesis (`SS3`)

The parametric bootstrapping approach implemented in **Stock Synthesis (SS3)** aims to simulate new datasets by sampling from the **observation model** used in the likelihood, conditional on the expected values predicted by the model. This procedure is useful for evaluating model performance and quantifying uncertainty in stock assessments.

#### Step 1: Calculate Expected Values

The model first calculates the **expected values** for each type of observation (abundance indices, length compositions, age compositions, discards, and tagging data). These values correspond to the model-predicted observations $\hat{y}_i$ that minimize the likelihood:

$$
\hat{y}_i = f(\theta)
$$

where $\theta$ represents the vector of estimated parameters.

These expected values are then used as the **mean** or **center** of the distributions from which new pseudo-observations are sampled.

#### Step 2: Simulate Observations Using Likelihood-Based Distributions

Each observation type is resampled using a distribution **corresponding to its likelihood function**. The sampling includes observation error and model-estimated variability (such as overdispersion or extra SD).

##### Abundance Indices

Abundance indices are typically assumed to follow a **log-normal distribution**:

$$
\log(I_i^{\text{boot}}) \sim \mathcal{N}\left( \log(\hat{I}_i), \sigma_i^2 \right)
$$

where:

* $I_i^{\text{boot}}$ is the bootstrapped index.
* $\hat{I}_i$ is the model-predicted index.
* $\sigma_i^2 = \text{Input CV}^2 + \text{Extra SD}^2 + \text{Variance Adjustment}$

Other error types (e.g., normal, t-distribution) may also be used depending on the `Errtype` setting in the control file.

> **Figure 1**: Sampling lognormal indices of abundance.

```{r fig1, fig.cap="Lognormal sampling of abundance index", echo=FALSE}
set.seed(42)
x <- rlnorm(1000, meanlog = log(1), sdlog = 0.3)
hist(x, breaks = 50, main = "Bootstrapped abundance index",
     xlab = "Index value", 
     col = "lightblue")
abline(v = mean(x), col = "red", lwd = 2)
```

##### Length and Age Compositions

Length and age composition data are sampled from a **multinomial distribution**:

$$
\mathbf{x}^{\text{boot}} \sim \text{Multinomial}(N_{\text{eff}}, \hat{\mathbf{p}})
$$

where:

* $\mathbf{x}^{\text{boot}}$ is the vector of bootstrapped counts.
* $N_{\text{eff}}$ is the effective sample size (possibly scaled via variance adjustments).
* $\hat{\mathbf{p}}$ are the expected proportions in each bin.



# Reults

## Testing Bias in Reference Model `s1` 

This code implements a parametric bootstrap specifically designed for stock assessment models, creating uncertainty estimates around wedge clam population dynamics. The process creates 100 bootstrap iterations where each iteration takes your base dataset and introduces controlled randomness into the length composition data, then runs the Stock Synthesis model on each perturbed dataset. This generates a distribution of possible outcomes that reflects the inherent uncertainty in your field data collection.
The bootstrap focuses on manipulating the length composition data from your clam stock assessment, which includes the size structure of your catches and population, the sample sizes representing the number of individuals measured in each sample, and the length frequency distributions showing proportions of different size classes. 

This is particularly important for clam assessments because size structure directly influences estimates of growth, mortality, and reproductive capacity. The randomization technique works by taking each length frequency observation and applying random multipliers between 0.9 and 1.1, which introduces plus or minus 10% variability around each observed proportion. It then resamples using a multinomial distribution to maintain realistic count structure, simulating the natural sampling variability you would see if you repeated your field sampling multiple times. This approach captures how uncertainty in size structure data propagates through to key management quantities like spawning stock biomass and recruitment estimates, giving you a realistic range of possible outcomes for your clam population assessment.



```{r eval=FALSE}
#### PASO 1: GENERAR ARCHIVOS BOOTSTRAP CON SS3
dir_test    <- here::here("s01")        # Reference model (2.5 MLS fixed)
# 1.1 Modificar el archivo starter.ss para generar bootstrap
starter_file <- SS_readstarter(file = file.path(dir_test,
                                               "starter.ss"),
                              verbose = FALSE)
# Cambiar el número de archivos de datos a producir (3 o más para bootstrap)
# 3 = archivo original + expected values + 1 bootstrap
# Para 100 bootstrap, usar 102
starter_file$N_bootstraps <- 102  # 100 bootstrap + original + expected values

# Escribir el starter modificado
SS_writestarter(starter_file, dir = dir_test, overwrite = TRUE)

# 1.2 Ejecutar SS3 para generar los archivos bootstrap
r4ss::run(
  dir = dir_test,
  exe = "../executable/ss3_opt_osx_arm64",
  extras = "-nox -nohess",
  skipfinished = FALSE,
  show_in_console = TRUE# change to true to watch the output go past
)
```


Esto generará archivos: `data_boot_001.ss`, `data_boot_002.ss`, ..., `data_boot_100.ss`

```{r eval=FALSE}
#### PASO 2: EJECUTAR MODELOS CON CADA ARCHIVO BOOTSTRAP
# 2.1 Crear directorio para resultados bootstrap
dir.create("s01/bootstrap_runs", showWarnings = FALSE)

# 2.2 Función para ejecutar un bootstrap individual
run_bootstrap <- function(boot_num, base_dir = "s01", output_dir = "bootstrap_runs") {
  
  # Crear directorio para este bootstrap
  boot_dir <- file.path(output_dir, paste0("boot_", sprintf("%03d", boot_num)))
  dir.create(boot_dir, showWarnings = FALSE, recursive = TRUE)
  
  # Copiar todos los archivos excepto data.ss
  files_to_copy <- list.files(base_dir, full.names = TRUE)
  files_to_copy <- files_to_copy[!grepl("data.ss$|data_boot_", basename(files_to_copy))]
  
  file.copy(from = files_to_copy, to = boot_dir, overwrite = TRUE)
  
  # Copiar el archivo bootstrap específico como data.ss
  boot_file <- file.path(base_dir, paste0("data_boot_", sprintf("%03d", boot_num), ".ss"))
  if (file.exists(boot_file)) {
    file.copy(from = boot_file, 
              to = file.path(boot_dir, "data.ss"), 
              overwrite = TRUE)
    # Ejecutar modelo
    system(paste0("cd ", boot_dir, " && ~/IEO/SA_Donax_trunculus/executable/ss3_opt_osx_arm64 -nox -nohess"))
    return(TRUE)
  } else {
    warning(paste("Bootstrap file", boot_file, "not found"))
    return(FALSE)
  }
}
```


```{r eval=FALSE}
# 2.3 Ejecutar todos los bootstrap (puede tomar tiempo)
set.seed(123)
n_bootstrap <- 100

# Ejecutar en paralelo si tienes muchos cores
library(parallel)
cl <- makeCluster(detectCores() - 1)
clusterEvalQ(cl, library(r4ss))

results <- parLapply(cl, 1:n_bootstrap, run_bootstrap)
stopCluster(cl)

# O ejecutar secuencialmente
# for (i in 1:n_bootstrap) {
#   cat("Running bootstrap", i, "of", n_bootstrap, "\n")
#   run_bootstrap(i)
# }
```



```{r eval=FALSE}
dir_test    <- here::here("s01")  
#### PASO 3: LEER Y COMPILAR RESULTADOS
# 3.1 Leer modelo base
base_model <- SS_output(dir = dir_test, verbose = FALSE, printstats = FALSE)

read_bootstrap_results <- function(boot_num, output_dir = "bootstrap_runs") {
  boot_dir <- file.path(output_dir, paste0("boot_", sprintf("%03d", boot_num)))
  
  tryCatch({
    model <- SS_output(dir = boot_dir, verbose = FALSE, printstats = FALSE)
    
    get_param <- function(label) {
      val <- model$parameters$Value[model$parameters$Label == label]
      if(length(val) == 0) return(NA_real_) else return(val)
    }
    # en esta lista agregar otras varibles o parametros
    list(
      bootstrap = boot_num,
      SSB = model$timeseries[model$timeseries$Yr >= 2003 & model$timeseries$Yr <= 2023, c("Yr", "SpawnBio")],
      recruitment = model$timeseries[model$timeseries$Yr >= 2003 & model$timeseries$Yr <= 2023, c("Yr", "Recruit_0")],
      depletion = model$derived_quants[model$derived_quants$Label == "Bratio_2028", "Value"],
      R0 = get_param("SR_LN(R0)"),
      M = get_param("NatM_p_1_Fem_GP_1"),
      h = get_param("SR_BH_steep")
    )
  }, error = function(e) {
    warning(paste("Error reading bootstrap", boot_num, ":", e$message))
    return(NULL)
  })
}

# Read data of boostrap
bootstrap_results <- lapply(1:n_bootstrap, read_bootstrap_results)
bootstrap_results <- bootstrap_results[!sapply(bootstrap_results, is.null)]
```


```{r eval=FALSE}
save(bootstrap_results, ssb_boot_df, recruit_boot_df, 
     ssb_base_df, recruit_base_df,
     file = "s01/bootstrap_summary.RData")
```


```{r}
# Carga el archivo
load("s01/bootstrap_summary.RData", verbose = TRUE)
```


```{r}
dir_test    <- here::here("s01")  
#### PASO 3: LEER Y COMPILAR RESULTADOS
# 3.1 Leer modelo base
base_model <- SS_output(dir = dir_test, verbose = FALSE, printstats = FALSE)
# compile SSB data
ssb_boot_df <- do.call(rbind, lapply(bootstrap_results, function(x) {
  if (!is.null(x$SSB)) {
    data.frame(x$SSB, bootstrap = x$bootstrap)
  }
}))
names(ssb_boot_df)[2] <- "SSB"

# compile Recruitment data
recruit_boot_df <- do.call(rbind, lapply(bootstrap_results, function(x) {
  if (!is.null(x$recruitment)) {
    data.frame(x$recruitment, bootstrap = x$bootstrap)
  }
}))
names(recruit_boot_df)[2] <- "Recruitment"

# Reference model
ssb_base_df <- base_model$timeseries[base_model$timeseries$Yr >= 2003 & 
                                       base_model$timeseries$Yr <= 2023, 
                                     c("Yr", "SpawnBio")]
names(ssb_base_df)[2] <- "SSB"

recruit_base_df <- base_model$timeseries[base_model$timeseries$Yr >= 2003 & 
                                           base_model$timeseries$Yr <= 2023, 
                                        c("Yr", "Recruit_0")]
names(recruit_base_df)[2] <- "Recruitment"
```


```{r}
# ====================================================================
# PASO 4: ANÁLISIS ESTADÍSTICO
# ====================================================================
# 4.1 Estadísticas resumidas por año
ssb_stats <- ssb_boot_df %>%
  group_by(Yr) %>%
  summarise(
    mean = mean(SSB, na.rm = TRUE),
    median = median(SSB, na.rm = TRUE),
    sd = sd(SSB, na.rm = TRUE),
    q025 = quantile(SSB, 0.025, na.rm = TRUE),
    q975 = quantile(SSB, 0.975, na.rm = TRUE),
    cv = sd/mean,
    .groups = 'drop'
  )

recruit_stats <- recruit_boot_df %>%
  group_by(Yr) %>%
  summarise(
    mean = mean(Recruitment, na.rm = TRUE),
    median = median(Recruitment, na.rm = TRUE),
    sd = sd(Recruitment, na.rm = TRUE),
    q025 = quantile(Recruitment, 0.025, na.rm = TRUE),
    q975 = quantile(Recruitment, 0.975, na.rm = TRUE),
    cv = sd/mean,
    .groups = 'drop'
  )

# 4.2 Parámetros derivados
derived_params <- data.frame(
  bootstrap = sapply(bootstrap_results, function(x) x$bootstrap),
  depletion = sapply(bootstrap_results, function(x) x$depletion),
  R0 = sapply(bootstrap_results, function(x) x$R0)
)

# Estadísticas de parámetros derivados
param_stats <- derived_params %>%
  summarise(
    depletion_mean = mean(depletion, na.rm = TRUE),
    depletion_sd = sd(depletion, na.rm = TRUE),
    depletion_q025 = quantile(depletion, 0.025, na.rm = TRUE),
    depletion_q975 = quantile(depletion, 0.975, na.rm = TRUE),
    R0_mean = mean(R0, na.rm = TRUE),
    R0_sd = sd(R0, na.rm = TRUE),
    R0_q025 = quantile(R0, 0.025, na.rm = TRUE),
    R0_q975 = quantile(R0, 0.975, na.rm = TRUE)
  )

kbl(param_stats)
```


```{r, fig.height=3, fig.width=10}
# ====================================================================
# PASO 5: VISUALIZACIÓN
# ====================================================================
# 5.1 Gráfico de SSB con formato uniforme
p_ssb <- ggplot() +
  geom_line(data = ssb_boot_df, 
            aes(x = Yr, y = SSB, group = bootstrap), 
            color = "gray", alpha = 0.1) +
  geom_ribbon(data = ssb_stats, 
              aes(x = Yr, ymin = q025, ymax = q975), 
              alpha = 0.2, fill = "gray80") +
  # Media bootstrap línea gris dashed
  geom_line(data = ssb_stats, 
            aes(x = Yr, y = mean), 
            color = "gray40", size = 1) +
  # Modelo base línea azul sólida
  geom_line(data = ssb_base_df, 
            aes(x = Yr, y = SSB), 
            color = "blue") +
  labs(x = "", y = "SSB") +
  scale_x_continuous(breaks = seq(2003,2023,2)) +
  theme_few() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# 5.2 Gráfico de Reclutamiento con mismo estilo
p_recruit <- ggplot() +
  geom_line(data = recruit_boot_df, 
            aes(x = Yr, y = Recruitment, group = bootstrap), 
            color = "gray", alpha = 0.1) +
  geom_ribbon(data = recruit_stats, 
              aes(x = Yr, ymin = q025, ymax = q975), 
              alpha = 0.2, fill = "gray80") +
  geom_line(data = recruit_stats, 
            aes(x = Yr, y = mean), 
            color = "gray40", size = 1) +
  geom_line(data = recruit_base_df, 
            aes(x = Yr, y = Recruitment), 
            color = "blue") +
  labs(x = "", y = "Recruit") +
  scale_x_continuous(breaks = seq(2003,2023,2)) +
  theme_few() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# 5.3 Histogramas de parámetros derivados
p_depletion <- ggplot(derived_params, aes(x = depletion)) +
  geom_histogram(bins = 30, alpha = 0.7, color= "lightgrey", 
                 fill = "lightblue") +
  geom_vline(xintercept = base_model$derived_quants[base_model$derived_quants$Label == "Bratio_2026", "Value"], 
             color = "red", size = 0.9) +
  labs(
       x = "Bratio", y = "Frecuency") +
  theme_few()
```


```{r, fig.height=3, fig.width=10}
# 5.4 Mostrar gráficos
ggpubr::ggarrange(p_ssb,
          p_recruit,
          p_depletion,
          ncol = 3,
          labels = c("A", "B", "C"))
```

Calcular desviación relativa entre bootstrap y modelo base por año

```{r }
# 1. 
ssb_devs <- ssb_boot_df %>%
  left_join(ssb_base_df, by = "Yr") %>%
  rename(ssb_boot = SSB.x, ssb_base = SSB.y) %>%
  mutate(rel_dev = (ssb_boot - ssb_base) / ssb_base)

# 2. Histograma por año
histboo <- ggplot(ssb_devs, aes(x = rel_dev)) +
  geom_histogram(bins = 30, fill = "grey", color = "darkgrey") +
  facet_wrap(~Yr, scales = "free_y",
             ncol=3) +
  geom_vline(xintercept = 0, color = "red") +
  labs(
    x = "SSB Relative Deviation (bootstrap - base) / base",
    y = "Frecuency",
    title = ""
  ) +
  theme_minimal()
histboo
```
There is a right (positive) skewness: many years exhibit long right tails, indicating that several bootstrap replicates overestimated the spawning stock biomass (SSB) compared to the base model. In other words, the bias tends to be positive. Some years show more bias than others; for example, 2013, 2016, and 2020 display distributions clearly shifted to the right, suggesting the model may be more sensitive in those years, possibly due to smaller sample sizes, changes in selectivity, high recruitment, or other factors worth exploring.

```{r }
# Extraer valores como escalares individuales
ssb_2023_val <- ssb_base_df$SSB[ssb_base_df$Yr == 2023]
recruit_2023_val <- recruit_base_df$Recruitment[recruit_base_df$Yr == 2023]
depletion_2023_val <- base_model$derived_quants$Value[base_model$derived_quants$Label == "Bratio_2023"]

ssb_boot_mean <- ssb_stats$mean[ssb_stats$Yr == 2023]
recruit_boot_mean <- recruit_stats$mean[recruit_stats$Yr == 2023]
depletion_boot_mean <- param_stats$depletion_mean

ssb_boot_sd <- ssb_stats$sd[ssb_stats$Yr == 2023]
recruit_boot_sd <- recruit_stats$sd[recruit_stats$Yr == 2023]
depletion_boot_sd <- param_stats$depletion_sd

ssb_boot_q025 <- ssb_stats$q025[ssb_stats$Yr == 2023]
recruit_boot_q025 <- recruit_stats$q025[recruit_stats$Yr == 2023]
depletion_boot_q025 <- param_stats$depletion_q025

ssb_boot_q975 <- ssb_stats$q975[ssb_stats$Yr == 2023]
recruit_boot_q975 <- recruit_stats$q975[recruit_stats$Yr == 2023]
depletion_boot_q975 <- param_stats$depletion_q975

comparison_table <- data.frame(
  Parameter = c("SSB_2023", "Recruit_2023", "Depletion_2023"),
  Base_Model = c(ssb_2023_val, recruit_2023_val, depletion_2023_val),
  Bootstrap_Mean = c(ssb_boot_mean, recruit_boot_mean, depletion_boot_mean),
  Bootstrap_SD = c(ssb_boot_sd, recruit_boot_sd, depletion_boot_sd),
  CI_Lower = c(ssb_boot_q025, recruit_boot_q025, depletion_boot_q025),
  CI_Upper = c(ssb_boot_q975, recruit_boot_q975, depletion_boot_q975)
)

print(comparison_table)

```


```{r eval=FALSE}
# Guardar resultados
write.csv(ssb_stats, "ssb_bootstrap_stats.csv", row.names = FALSE)
write.csv(recruit_stats, "recruitment_bootstrap_stats.csv", row.names = FALSE)
write.csv(comparison_table, "bootstrap_comparison.csv", row.names = FALSE)
```


## Distribucion de parametros

```{r }
# Parámetros base:
h_base <- base_model$parameters$Value[base_model$parameters$Label == "SR_BH_steep"]
R0_base <- base_model$parameters$Value[base_model$parameters$Label == "SR_LN(R0)"]

# Extraer de la lista bootstrap_results
h_boot <- sapply(bootstrap_results, function(x) x$h)
R0_boot <- sapply(bootstrap_results, function(x) x$R0)

```


```{r }
plot_histogram <- function(boot_values, base_value, param_name) {
  df <- data.frame(value = boot_values)
  ggplot(df, aes(x = value)) +
    geom_histogram(binwidth = diff(range(boot_values))/30, fill = "lightgrey", color = "grey") +
    geom_vline(xintercept = base_value, color = "red") +
    labs(x = param_name,
         y = "Frequency") +
    theme_few()
}

p_h <- plot_histogram(h_boot, h_base, "Steepness (h)")
p_R0 <- plot_histogram(R0_boot, R0_base, "R0")

ggarrange(p_h, p_R0,
          ncol = 2, nrow = 1,
          labels = c("A", "B"))
```
```{r }
# Unir por año
ssb_compare <- left_join(ssb_boot_df, ssb_base_df, by = "Yr", suffix = c("_boot", "_base"))

# Calcular el RMSE global (todos los bootstraps juntos)
rmse <- ssb_compare %>%
  mutate(error = SSB_boot - SSB_base) %>%
  summarise(RMSE = sqrt(mean(error^2))) %>%
  pull(RMSE)

cat("RMSE:", round(rmse, 3), "\n")

```


```{r }
# Crear serie base con lag para modelo naive
ssb_base_df <- ssb_base_df %>%
  arrange(Yr) %>%
  mutate(SSB_lag1 = lag(SSB))

# Calcular MAE del modelo naive (base)
mae_naive <- mean(abs(diff(ssb_base_df$SSB)), na.rm = TRUE)

# Calcular MAE entre cada bootstrap y el modelo base
mae_model <- ssb_compare %>%
  mutate(abs_error = abs(SSB_boot - SSB_base)) %>%
  summarise(mae = mean(abs_error, na.rm = TRUE)) %>%
  pull(mae)

# Calcular MASE
mase <- mae_model / mae_naive

cat("MASE:", round(mase, 3), "\n")

```

# Discussion

To assess the robustness of model estimates and diagnose potential sources of structural bias, we conducted a non-parametric bootstrap analysis focusing on uncertainty in the length composition data. A total of 100 bootstrap replicates were generated by resampling the observed length compositions with replacement. Each replicate was used to refit the Stock Synthesis (SS3) model, maintaining all structural assumptions constant. The resulting bootstrap runs provided distributions of spawning stock biomass (SSB), recruitment, and depletion (Bratio), which were compared against the reference model.

Panel **A** of the figure illustrates the trajectories of spawning stock biomass (SSB) from 2003 to 2023 for each bootstrap replicate (gray lines), with the reference model shown in blue. Panel **B** shows analogous results for recruitment. Both panels reveal that during the initial years of the time series (2003–2017), there is substantial variability across bootstrap runs, indicating greater model sensitivity to resampled input data. This increased uncertainty and apparent bias in the early years is likely due to the **absence of fishery-independent index data and length composition data before 2018**. Consequently, model estimates for the early period are driven primarily by priors and assumptions (e.g., initial equilibrium conditions, natural mortality, and selectivity patterns), rather than empirical data. This results in **higher inter-replicate variability and bias**, particularly evident in SSB estimates.

In contrast, from **2018 onward**, when both length composition and index data are available, the bootstrap trajectories converge more closely around the reference model, indicating greater stability and less bias in estimated trends. This improvement highlights the critical role of observational data in constraining model outputs.

Panel **C** displays the distribution of depletion (Bratio) in 2028 across bootstrap replicates (histogram), with the reference model’s estimate indicated by the vertical red line. The histogram shows a moderately right-skewed distribution centered around 0.49–0.50, while the reference model estimate is lower than most replicates (around 0.43). This suggests that, under data resampling, the model tends to produce slightly **higher estimates of depletion** than the reference scenario, indicating a **possible downward bias in the reference model’s Bratio**. Such discrepancy may reflect structural assumptions, data weighting, or lack of informative data in the final projected year.

# Conclusion

This bootstrap analysis reveals that model estimates prior to 2018 are more susceptible to bias due to the lack of informative input data. The presence of high inter-replicate variability in early years underscores the importance of data availability in constraining model estimates. Moreover, the tendency for the reference model to estimate a lower Bratio than the bootstrap median suggests potential structural bias that should be further investigated, possibly through sensitivity analysis or alternative model configurations.

# References
